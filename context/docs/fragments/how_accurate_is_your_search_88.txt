# How Accurate is Your Search? [Source Link](https://docs.videodb.io/how-accurate-is-your-search-88)

![videodb](https://codaio.imgix.net/workspaces/ws-jizMKG73gK/blobs/customIcons/1a6d553a-3676-494e-8f3b-fd666614f459?fit=fill&fill=solid&w=128&h=128&fm=gif&bg=0FFF&fill-color=0FFF)

VideoDB Documentation

Pages

[![](https://cdn.coda.io/icons/svg/color/align-center.svg)\\
\\
Welcome to VideoDB Docs](https://docs.videodb.io/)

[![](https://cdn.coda.io/icons/svg/color/quick-mode-on.svg)\\
\\
Quick Start Guide](https://docs.videodb.io/quick-start-guide-38)

[![icon picker](https://cdn.coda.io/icons/svg/color/wash-your-hands.svg)\\
\\
How Accurate is Your Search?](https://docs.videodb.io/how-accurate-is-your-search-88)

[![](https://cdn.coda.io/icons/svg/color/video-call.svg)\\
\\
Video Indexing Guide](https://docs.videodb.io/video-indexing-guide-101)

[![](https://cdn.coda.io/icons/svg/color/clear-search.svg)\\
\\
Semantic Search](https://docs.videodb.io/semantic-search-89)

[![](https://cdn.coda.io/icons/svg/color/binders-folder.svg)\\
\\
Collections](https://docs.videodb.io/collections-68)

[![](https://cdn.coda.io/icons/svg/color/magazine.svg)\\
\\
Public Collections](https://docs.videodb.io/public-collections-102)

[![](https://cdn.coda.io/icons/svg/color/callback.svg)\\
\\
Callback Details](https://docs.videodb.io/callback-details-66)

[![](https://cdn.coda.io/icons/svg/color/closed-captioning.svg)\\
\\
Ref: Subtitle Styles](https://docs.videodb.io/ref-subtitle-styles-57)

[![](https://cdn.coda.io/icons/svg/color/customer-support.svg)\\
\\
Language Support](https://docs.videodb.io/language-support-79)

[![](https://cdn.coda.io/icons/svg/color/closed-captioning.svg)\\
\\
Guide: Subtitles](https://docs.videodb.io/guide-subtitles-73)

[![](https://cdn.coda.io/icons/svg/color/asteroid.svg)\\
\\
Visual Search and Indexing](https://docs.videodb.io/visual-search-and-indexing-80)

[![](https://cdn.coda.io/icons/svg/color/clear-search.svg)\\
\\
Multimodal Search](https://docs.videodb.io/multimodal-search-90)

[![](https://cdn.coda.io/icons/svg/color/e-learning.svg)\\
\\
Dynamic Video Streams](https://docs.videodb.io/dynamic-video-streams-44)

[![director-light](https://codaio.imgix.net/workspaces/ws-jizMKG73gK/blobs/customIcons/6bc288c2-982b-4a97-a402-8da53aeaa236?fit=fill&fill=solid&w=128&h=128&fm=gif&bg=0FFF&fill-color=0FFF)\\
\\
Director - Video Agent Framework](https://docs.videodb.io/director-video-agent-framework-98)

[![github](https://codaio.imgix.net/workspaces/ws-jizMKG73gK/blobs/customIcons/ac14f3ef-daa1-4b6e-aba5-af11f11b8372?fit=fill&fill=solid&w=128&h=128&fm=gif&bg=0FFF&fill-color=0FFF)\\
\\
Open Source Tools](https://docs.videodb.io/open-source-tools-94)

[![](https://cdn.coda.io/icons/svg/color/book-and-pencil.svg)\\
\\
Examples and Tutorials](https://docs.videodb.io/examples-and-tutorials-35)

[![](https://cdn.coda.io/icons/svg/color/centre-of-gravity.svg)\\
\\
Edge of Knowledge](https://docs.videodb.io/edge-of-knowledge-10)

[![videodb](https://codaio.imgix.net/workspaces/ws-jizMKG73gK/blobs/customIcons/1a6d553a-3676-494e-8f3b-fd666614f459?fit=fill&fill=solid&w=128&h=128&fm=gif&bg=0FFF&fill-color=0FFF)\\
\\
Building World's First Video Database](https://docs.videodb.io/building-worlds-first-video-database-25)

[![](https://cdn.coda.io/icons/svg/color/the-dragon-team.svg)\\
\\
Team](https://docs.videodb.io/team-46)

[![](https://cdn.coda.io/icons/svg/color/like.svg)\\
\\
Customer Love](https://docs.videodb.io/customer-love-42)

[![](https://cdn.coda.io/icons/svg/color/llama.svg)\\
\\
Temp Doc](https://docs.videodb.io/temp-doc-54)

Quick Start Guide

# ![icon picker](https://cdn.coda.io/icons/svg/color/wash-your-hands.svg)         How Accurate is Your Search?

### Introduction

When you index your data and retrieve it with certain parameters, how do you measure the effectiveness of your search? This is where search evaluation comes in. By using test data, queries, and their results, you can assess the performance of indexes, search parameters, and other related factors. This evaluation helps you understand how well your search system is working and identify areas for improvement.

### Example

To keep it super simple let‚Äôs use a

[countdown video](https://www.youtube.com/watch?v=tWoo8i_VkvI)

of 30 seconds.

Loading‚Ä¶

We can imagine information in video indexed as documents which are ‚Äútimestamps + some textual information‚Äù describing the visuals as there is no audio in this video‚Äù.

We can use the structure as

timestamp : (start, end ),description: ‚Äústring‚Äù

So, if we use index\_scenes function

At (1, 2) - 29 seconds is displayed

At (2, 3) - 28 seconds is displayed

...

This continues until:

At (29, 30) - 1 second is displayed

### Ground Truth

It is the the ideal expected result. To evaluate the performance of search we need some test queries and the expected results.

Let's say for the query "Six" the expected result documents are at the following timestamps:

We will call this list of timestamps our ground truth for the query "Six."

### Evaluation Metrics

To evaluate the effectiveness of our search functionality, we'll can experiment with our query "Six" with various search parameters. üìä

The search results can be categorized as follows:

Retrieved Documents üîç:

Retrieved Relevant Documents: Matches our ground truth ‚úÖ

Retrieved Irrelevant Documents: Don't match our ground truth ‚ùå

Non-Retrieved Documents üö´:

Non-Retrieved Relevant Documents: In our ground truth but not in results üòï

Non-Retrieved Irrelevant Documents: Neither in ground truth nor results üëç

We can further classify these categories in terms of search accuracy:

True Positives (TP) üéØ: Retrieved Relevant Documents

We wanted them, and we got them üôå

False Positives (FP) üé≠: Retrieved Irrelevant Documents

We didn't want them, but we got them ü§î

False Negatives (FN) üò¢: Non-Retrieved Relevant Documents

We wanted them, but we didn't get them üòì

True Negatives (TN) üö´: Non-Retrieved Irrelevant Documents

We didn't want them, and we didn't get them üëå

üí° This classification helps us assess the precision and recall of our search algorithm, enabling further optimization.

### Accuracy

Accuracy measures how well our search algorithm retrieves required documents while excluding irrelevant ones. It can be calculated as follows:

In other words, accuracy is the ratio of correctly classified documents (both retrieved relevant and non-retrieved irrelevant) to the total number of documents. üìä

To get a more comprehensive evaluation of search performance, it's crucial to consider other metrics such as precision, recall, and F1-score in addition to accuracy. üí°üî¨

### Precision and Recall

Precision is percentage of relevant retrieved docs out of all retrieved docs. It answers the question: "Of the documents our search returned, how many were actually relevant?"

Recall indicates the percentage of relevant documents that were successfully retrieved. It addresses the question: "Out of all the relevant documents, how many did our search find?" üîç

### The Precision-Recall Trade-off

These metrics often have an inverse relationship, leading to a trade-off:

Recall üìà:

Measures the model's ability to find all relevant cases in a dataset.

Increases or remains constant as more documents are retrieved.

Never decreases with an increase in retrieved documents.

Precision üìâ:

Refers to the proportion of correct positive identifications.

Typically decreases as more documents are retrieved.

Drops due to increased likelihood of including false positives.

### Search in VideoDB

Let‚Äôs understand the search interface provided by VideoDB and measure results with the above metric.

This function performs a search on video content with various customizable parameters:

query: The search query string.

search\_type: Determines the search method. Keyword search on single video level returns all the documents .

SearchType.semantic(default): For question-answering queries. ( across 1000s of videos/ collection ) Checkout

[![](https://cdn.coda.io/icons/svg/color/clear-search.svg)\\
Semantic Search](https://docs.videodb.io/semantic-search-89)

for detailed understanding.

SearchType.keyword: Matches exact occurrences where the given query is present as a sub-string (single video only).

index\_type: Specifies the index to search:

IndexType.spoken\_word(default): Searches spoken content.

IndexType.scene: Searches visual content.

result\_threshold: Initial filter for top N matching documents (default: 5).

score\_threshold: Absolute threshold filter for relevance scores (default: 0.2).

dynamic\_score\_percentage: Adaptive filtering mechanism:

Useful when there is a significant gap between top results and tail results after score\_threshold filter. Retains top x% of the score range.

Calculation: dynamic\_threshold = max\_score \- (range \\* dynamic\_score\_percentage)

default: 20%

This interface allows for flexible and precise searching of video content, with options to fine-tune result filtering based on relevance scores and dynamic thresholds.

### Experiment

Follow this notebook to explore experiments on fine-tuning search results and gain a deeper understanding of the methods involved

[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/video-db/videodb-cookbook/blob/main/guides/VideoDB_Search_and_Evaluation.ipynb)

Here‚Äôs a basic outcome of the default settings for both search types on the query "six" for the above video:

1\. Semantic Search Default:

2\. Keyword Search:

### Outcome

As you can see, keyword search is best suited for queries like "teen" and "six." However, if the queries are in natural language, such as "find me a 6" then semantic search is more appropriate.

Keyword search would struggle to find relevant results for such natural language queries.

### Search + LLM

For complex queries like "Find me all the numbers greater than six" a basic search will not work effectively since it merely matches the query with documents in vector space and returns the matching documents.

In such cases, you can apply a loose filter to get all the documents that match the query. However, you will need to add an additional layer of intelligence using a Large Language Model (LLM). The matched documents can then be passed to the LLM to curate a response that accurately answers the query.

Introduction

Example

Ground Truth

Evaluation Metrics

Accuracy

Precision and Recall

The Precision-Recall Trade-off

Search in VideoDB

Experiment

Outcome

Search + LLM

Want to print your doc?

This is not the way.

![](https://cdn.coda.io/assets/2462459f3eb1/img/import_google_docs.png)

Try clicking the ‚ãØ next to your doc name or using a keyboard shortcut (

CtrlP

) instead.


---

